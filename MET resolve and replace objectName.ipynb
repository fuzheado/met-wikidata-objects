{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7700f54a-bd80-4318-9c95-12659aa647b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce1bec7d8024f32baf441a090bd4613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q116297331: 242177, Statuette of a lion -> Q16738862\n",
      "Q116293073: 246424, Statuette of a youth -> Q16738862\n",
      "Q116287353: 251629, Statuette of a warrior, 10 -> Q16738862\n",
      "Q116290591: 249180, Statuette of a woman reclining -> Q16738862\n",
      "Q116287941: 257430, Statuette of a bearded man -> Q16738862\n",
      "Q116290458: 241173, Statuette of a woman carrying a jar -> Q16738862\n",
      "Q116390387: 967, Brandy tumbler -> Q16180001\n",
      "Q116297691: 242066, Statuette of a priest (?) wearing bull mask -> Q16738862\n",
      "Q116287364: 251618, Statuette of Athena, 5 -> Q16738862\n",
      "Q116287202: 251656, Statuette of Poseidon, 4 -> Q16738862\n",
      "Q116390392: 969, Brandy tumbler -> Q16180001\n",
      "Q116403314: 251615, Statuette of Athena -> Q16738862\n",
      "Q116402375: 247574, Statuette of standing woman -> Q16738862\n",
      "Q116390375: 965, Brandy tumbler -> Q16180001\n",
      "Q116291641: 254769, Statuette of a goddess -> Q16738862\n",
      "Q116390369: 962, Brandy tumbler -> Q16180001\n",
      "Q116390389: 968, Brandy tumbler -> Q16180001\n",
      "Q116390371: 964, Brandy tumbler -> Q16180001\n",
      "Q116390370: 963, Brandy tumbler -> Q16180001\n",
      "Q116297728: 241327, Statuette of a horse and rider -> Q16738862\n",
      "Q116294176: 249036, Statuette of Herakles, drunken -> Q16738862\n",
      "Q116287330: 255432, Statuette of standing woman -> Q16738862\n",
      "Finished\n",
      "Total examined: 1000\n",
      "Replaced: 22\n"
     ]
    }
   ],
   "source": [
    "# Resolve and replace Wikidata items about Met's objects\n",
    "\n",
    "# Script to resolve and replace P31 statements for Met objects in Wikidata\n",
    "# Start with getting all P31 -> item of collection or exhibition (Q18593264)\n",
    "# Assume that there is a qualifier such as object named as (P1932) -> \"Halberd\"\n",
    "# (A separate bot has been designed to use Met CSV database to add these)\n",
    "\n",
    "# SPARQL query to list these generic objects that have P1932 set:\n",
    "# https://w.wiki/6Ndi\n",
    "\n",
    "# This script will use two crosswalk databases, stored on Wikidata:\n",
    "# https://www.wikidata.org/wiki/Wikidata:GLAM/Metropolitan_Museum_of_Art/glamingest/objectName\n",
    "# https://www.wikidata.org/wiki/Wikidata:GLAM/Metropolitan_Museum_of_Art/glamingest/objectName_regex\n",
    "\n",
    "# A report on a run done on February 24, 2023: https://gitlab.wikimedia.org/-/snippets/60\n",
    "\n",
    "# Code to read in a wikitable of a crosswalk database for Met object name to Wikidata item matching\n",
    "# From: https://github.com/fuzheado/glamingest/blob/master/pywikibottest.py\n",
    "\n",
    "# For running in JupyterLab at https://paws.wmcloud.org\n",
    "# !pip install wikitables\n",
    "# !pip install tabulate\n",
    "\n",
    "import pywikibot\n",
    "import pandas as pd\n",
    "from pywikibot import pagegenerators as pg\n",
    "from pywikibot.data.sparql import SparqlQuery\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import mwparserfromhell as mwp\n",
    "\n",
    "from tabulate import tabulate\n",
    "from wikitables import import_tables, WikiTable\n",
    "from wikitables.util import ftag\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "crosswalk_page_name = u'Wikidata:GLAM/Metropolitan_Museum_of_Art/glamingest/objectName'\n",
    "crosswalk_regex_page_name = u'Wikidata:GLAM/Metropolitan Museum of Art/glamingest/objectName regex'\n",
    "wikidata_api_url = 'https://www.wikidata.org/w/api.php'\n",
    "\n",
    "def import_tables_from_wikitext(wikitext, title=None):\n",
    "    # Set default title value\n",
    "    if title is None:\n",
    "        title = 'generic'\n",
    "\n",
    "    body = wikitext\n",
    "\n",
    "    ## parse for tables\n",
    "    raw_tables = mwp.parse(body).filter_tags(matches=ftag('table'))\n",
    "\n",
    "    def _table_gen():\n",
    "        for idx, table in enumerate(raw_tables):\n",
    "            name = '%s[%s]' % (title, idx)\n",
    "            yield WikiTable(name, table)\n",
    "\n",
    "    return list(_table_gen())\n",
    "\n",
    "def import_tables_from_url(api_url, title):\n",
    "    params = {'prop': 'revisions',\n",
    "              'format': 'json',\n",
    "              'action': 'query',\n",
    "              'explaintext': '',\n",
    "              'titles': title,\n",
    "              'rvprop': 'content'}\n",
    "\n",
    "    r = requests.get(api_url, params)\n",
    "    r.raise_for_status()\n",
    "    pages = r.json()[\"query\"][\"pages\"]\n",
    "\n",
    "    # use key from first result in 'pages' array\n",
    "    pageid = list(pages.keys())[0]\n",
    "    if pageid == '-1':\n",
    "        raise ArticleNotFound('no matching articles returned')\n",
    "\n",
    "    page = pages[pageid]\n",
    "    body = page['revisions'][0]['*']\n",
    "\n",
    "    return import_tables_from_wikitext(body, page['title'])\n",
    "\n",
    "\n",
    "def import_tables_from_url_full(api_url, title):\n",
    "    params = {'prop': 'revisions',\n",
    "              'format': 'json',\n",
    "              'action': 'query',\n",
    "              'explaintext': '',\n",
    "              'titles': title,\n",
    "              'rvprop': 'content'}\n",
    "\n",
    "    r = requests.get(api_url, params)\n",
    "    r.raise_for_status()\n",
    "    pages = r.json()[\"query\"][\"pages\"]\n",
    "\n",
    "    # use key from first result in 'pages' array\n",
    "    pageid = list(pages.keys())[0]\n",
    "    if pageid == '-1':\n",
    "        raise ArticleNotFound('no matching articles returned')\n",
    "\n",
    "    page = pages[pageid]\n",
    "    body = page['revisions'][0]['*']\n",
    "\n",
    "    ## parse for tables\n",
    "    raw_tables = mwp.parse(body).filter_tags(matches=ftag('table'))\n",
    "\n",
    "    def _table_gen():\n",
    "        for idx, table in enumerate(raw_tables):\n",
    "            name = '%s[%s]' % (page['title'], idx)\n",
    "            yield WikiTable(name, table)\n",
    "\n",
    "    return list(_table_gen())\n",
    "\n",
    "\n",
    "def cross_lookup_object(in_df:pd.DataFrame, objectName:str, regex_search=False):\n",
    "    '''\n",
    "    Lookup a Met object name in crosswalk database that is a Dataframe\n",
    "    Should have the following columns: Object Name, qid, extrastatement, extraqualifier (mostly unused)\n",
    "    '''\n",
    "    return_dict = {}\n",
    "    found_qids = None\n",
    "    \n",
    "    if regex_search:\n",
    "        for index, row in in_df.iterrows():\n",
    "            regex = row['Object Name']\n",
    "            if re.match(regex, objectName):\n",
    "                found_qids = [item for item in [row[1], row[2]] if isinstance(item, str)]\n",
    "    else:\n",
    "        # Make new DF from crosswalk matches to objectName, but may be more than one match or qid+extrastatement\n",
    "        sliced_df = in_df[in_df['Object Name'] == objectName]\n",
    "        # Make list out of qids, with possible nan values\n",
    "        found_qids = sliced_df['qid'].values.tolist() + sliced_df['extrastatement'].values.tolist()\n",
    "        # Clean the list of NaN values\n",
    "        found_qids = [item for item in found_qids if isinstance(item, str)]\n",
    "\n",
    "    if found_qids:\n",
    "        return_dict['qid'] = found_qids\n",
    "        # TOOD: handle possible qualifiers\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def cross_lookup_object_combined(exact_df:pd.DataFrame, regex_df:pd.DataFrame, objectName:str):\n",
    "    '''\n",
    "    Combined search searches two different dataframes\n",
    "        exact_df - for exact matches\n",
    "        regex_df - for regular expression matches\n",
    "    '''\n",
    "    result = cross_lookup_object(exact_df, objectName)\n",
    "    if not result:\n",
    "        result = cross_lookup_object(regex_df, objectName, regex_search=True)\n",
    "    return result\n",
    "\n",
    "def stringify_pywikibot_target(intarget):\n",
    "    '''\n",
    "    Take pywikibot target object and return a string version, or QID, or printed date object\n",
    "    '''\n",
    "    returnstring = ''\n",
    "    # print ('Type', type(intarget))\n",
    "    if isinstance(intarget, str):\n",
    "        return intarget\n",
    "    elif isinstance(intarget, pywikibot.page._wikibase.ItemPage):\n",
    "        return intarget.id\n",
    "    elif isinstance(intarget, pywikibot.WbTime):\n",
    "        return str(intarget)\n",
    "    return None\n",
    "\n",
    "def retrieve_claim_propqual(item, inclaimprop, inclaimtarget=None, inqualprop=None, inqualvalue=None):\n",
    "    ''' Retrieve an entire Wikidata claim if property/qualifier match \n",
    "        Only works with returning strings or items, which get returned as QIDs\n",
    "    '''\n",
    "    returnlist = []\n",
    "\n",
    "    item.get(force=True)\n",
    "    if not item.claims.get(inclaimprop):\n",
    "        return returnlist\n",
    "    for statement in item.claims[inclaimprop]:\n",
    "        if not inclaimtarget:\n",
    "            returnlist += [stringify_pywikibot_target(statement.target)]\n",
    "        elif statement.target.id == inclaimtarget:\n",
    "            # Process qualifiers, if they exist\n",
    "            if inqualprop and inqualprop in statement.qualifiers:\n",
    "                for qual in statement.qualifiers[inqualprop]: #iterate over all P1932\n",
    "                    returnqualstring = stringify_pywikibot_target(qual.target)\n",
    "                    if inqualvalue:\n",
    "                        if inqualvalue == returnqualstring:\n",
    "                            returnlist.append(returnqualstring)\n",
    "                    else:\n",
    "                        returnlist.append(returnqualstring)\n",
    "            elif not inqualprop:\n",
    "                # inclaimprop-inclaimtarget triple matched, but no qualifier specified\n",
    "                # Then just return the inclaimtarget QID\n",
    "                returnlist.append(stringify_pywikibot_target(statement.target))\n",
    "    return returnlist\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load crosswalk tables via API call, JSON\n",
    "    cross_tables = import_tables_from_url(wikidata_api_url, crosswalk_page_name)\n",
    "    cross_regex_tables = import_tables_from_url(wikidata_api_url, crosswalk_regex_page_name)\n",
    "\n",
    "    # Load as a Dataframe, but replace QID with qid as column name\n",
    "    cross_df = pd.read_json(cross_tables[0].json()).rename(columns={'QID': 'qid'}).replace(r'^\\s*$', np.nan, regex=True)\n",
    "    cross_regex_df = pd.read_json(cross_regex_tables[0].json()).rename(columns={'QID': 'qid'}).replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    # print (cross_df.sample(3))\n",
    "    # print (cross_regex_df.sample(3))\n",
    "    # print ('Test cross_lookup_object_combined')\n",
    "    # testobjects = [\n",
    "    #     'Foo', 'Bar', 'Jar', 'Jug', 'Wine cooler', 'Linen fragment', 'Figurine of Sa', 'Jug with waterspout'\n",
    "    # ]\n",
    "    # for obj in testobjects:\n",
    "    #     print (obj, cross_lookup_object_combined(cross_df, cross_regex_df, obj))\n",
    "\n",
    "    # Do SPARQL query to grab all entries of P31->Q18593264 and no qualifier\n",
    "    # Return QID, Met ID\n",
    "    # Create generator\n",
    "    QUERY = '''\n",
    "    SELECT ?item ?metid WHERE {\n",
    "      ?item wdt:P31 wd:Q18593264 .\n",
    "      ?item wdt:P3634 ?metid .\n",
    "    } ORDER BY RAND() LIMIT 1000\n",
    "    '''\n",
    "\n",
    "    # For SPARQL generator\n",
    "    wikidata_site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = wikidata_site.data_repository()\n",
    "    generator = pg.WikidataSPARQLPageGenerator(QUERY, site=wikidata_site)\n",
    "\n",
    "    # Other method to use SPARQL in one shot\n",
    "    # wikiquery = SparqlQuery()\n",
    "    # data = wikiquery.select(QUERY)\n",
    "\n",
    "    total_counter = 0\n",
    "    replaced_counter = 0\n",
    "\n",
    "    pbar = tqdm(generator)\n",
    "    for item in pbar:\n",
    "        object_name = None\n",
    "        matched_qids = None\n",
    "        generic_statement = None\n",
    "\n",
    "        # Grab Met ID, which should return exactly one value\n",
    "        metidlist = retrieve_claim_propqual(item, 'P3634')\n",
    "        if len(metidlist) != 1:\n",
    "            tqdm.write(f\"{item.id}: Error, Met ID should be exactly one. Instead: {metid}\")\n",
    "            continue\n",
    "\n",
    "        metid = metidlist[0]  # Extract the lone Met ID\n",
    "\n",
    "        total_counter += 1\n",
    "        \n",
    "        # Retrieve object name from qualifier\n",
    "        # NOW DONE MORE EFFICIENTLY BELOW\n",
    "        # object_result = retrieve_claim_propqual(item, 'P31', 'Q18593264', 'P1932')\n",
    "        # object_name = object_result[0] if len(object_result) == 1 else None\n",
    "\n",
    "        #Grab Wikidata P31 claims\n",
    "        item.claims.get('P31')\n",
    "        for statement in item.claims.get('P31'):  # Get instance of\n",
    "            if statement.target.id == 'Q18593264': # See if it's item of collection or exhibition\n",
    "                # Check for existing one\n",
    "                if 'P1932' in statement.qualifiers:\n",
    "                    for qual in statement.qualifiers['P1932']: #iterate over all\n",
    "                        object_name = stringify_pywikibot_target(qual.target)\n",
    "\n",
    "                # Output status message\n",
    "                pbar.set_postfix_str(f\"{item.id}: {metid}, {object_name}\")\n",
    "\n",
    "                # Lookup in exact/regex crosswalks for a P31 mapping\n",
    "                lookup_result = cross_lookup_object_combined(cross_df, cross_regex_df, object_name)\n",
    "                matched_qids = lookup_result.get('qid')\n",
    "                generic_statement = statement # Remember this statement so we can remove later\n",
    "\n",
    "        # Add new statement specific for P31, if matched\n",
    "        if not matched_qids: \n",
    "            continue # Skip if no match\n",
    "\n",
    "        for qid in matched_qids:\n",
    "            tqdm.write(f\"{item.id}: {metid}, {object_name} -> {qid}\")\n",
    "\n",
    "            # Add new P31 specific claim\n",
    "            new_statement = pywikibot.Claim(repo, 'P31')\n",
    "            target = pywikibot.ItemPage(repo, qid)\n",
    "            new_statement.setTarget(target)\n",
    "            item.addClaim(new_statement)\n",
    "\n",
    "            # Add qualifier with objectName string from Met\n",
    "            qualifier = pywikibot.Claim(repo, 'P1932')\n",
    "            qualifier.setTarget(object_name)\n",
    "            new_statement.addQualifier(qualifier) # summary=u'Adding a qualifier.'\n",
    "\n",
    "        # Remove generic statement\n",
    "        if generic_statement:\n",
    "            item.removeClaims(generic_statement)\n",
    "            replaced_counter += 1\n",
    "\n",
    "    # Output final report\n",
    "    tqdm.write(f\"Finished\")\n",
    "    tqdm.write(f\"Total examined: {total_counter}\")\n",
    "    tqdm.write(f\"Replaced: {replaced_counter}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
